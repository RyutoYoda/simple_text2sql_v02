import streamlit as st
import pandas as pd
import duckdb
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import re
import os
import base64
from openai import OpenAI
import faiss
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(page_title="Vizzye", layout="wide")
st.title("ğŸ§ Vizzy")

# ãƒ­ã‚´ç”»åƒè¡¨ç¤º
def load_image(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()

image_path = "vizzy_logo.png"
if os.path.exists(image_path):
    image_base64 = load_image(image_path)
    st.markdown(
        f"""<div style="text-align: center;">
        <img src="data:image/png;base64,{image_base64}" alt="image" style="width: 100%;"/>
        </div>""",
        unsafe_allow_html=True
    )

# èª¬æ˜
with st.expander("Vizzyã¨ã¯â”", expanded=False):
    st.markdown("""
**Vizzy** ã¯ã€è‡ªç„¶è¨€èªã§ãƒ‡ãƒ¼ã‚¿ã«è³ªå•ã§ãã‚‹ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã‚¢ãƒ—ãƒªã§ã™ã€‚  
CSV / Parquet / BigQuery / Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

**æ–°æ©Ÿèƒ½**: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚‚å¯èƒ½ã§ã™ï¼
- ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚„è‡ªç”±è¨˜è¿°ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªå‹•ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- 3æ¬¡å…ƒæ•£å¸ƒå›³ã§å¯è¦–åŒ–
- å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ„è¦‹ã‚’è‡ªå‹•è¦ç´„
- å°‘æ•°æ´¾ã®æ„è¦‹ã‚‚è¦‹é€ƒã—ã¾ã›ã‚“
""")

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
source = st.selectbox("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ", ["ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«", "BigQuery", "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"])

df = None

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
if source == "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«":
    uploaded_file = st.file_uploader("ğŸ“„ CSVã¾ãŸã¯Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv", "parquet"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_parquet(uploaded_file)

# BigQuery
elif source == "BigQuery":
    sa_file = st.file_uploader("ğŸ” BigQueryã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="json", key="bq")
    if sa_file:
        with open("temp_bq.json", "wb") as f:
            f.write(sa_file.getbuffer())
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "temp_bq.json"

        from google.cloud import bigquery
        try:
            client = bigquery.Client()
            datasets = list(client.list_datasets())
            dataset_names = [d.dataset_id for d in datasets]
            selected_dataset = st.selectbox("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", dataset_names)
            if selected_dataset:
                tables = list(client.list_tables(selected_dataset))
                table_names = [t.table_id for t in tables]
                selected_table = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«", table_names)
                if selected_table:
                    full_table_id = f"{client.project}.{selected_dataset}.{selected_table}"
                    df = client.query(f"SELECT * FROM `{full_table_id}` LIMIT 1000").to_dataframe()
        except Exception as e:
            st.error(f"BigQueryã‚¨ãƒ©ãƒ¼: {e}")

# Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ
elif source == "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ":
    sa_file = st.file_uploader("ğŸ” ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="json", key="sheet")
    if sa_file:
        with open("temp_sheet.json", "wb") as f:
            f.write(sa_file.read())
        import gspread
        try:
            gc = gspread.service_account(filename="temp_sheet.json")
            sheet_url = st.text_input("ğŸ“„ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URLã‚’å…¥åŠ›")
            if sheet_url:
                sh = gc.open_by_url(sheet_url)
                worksheet_names = [ws.title for ws in sh.worksheets()]
                selected_ws = st.selectbox("ğŸ§¾ ã‚·ãƒ¼ãƒˆã‚’é¸æŠ", worksheet_names)
                if selected_ws:
                    ws = sh.worksheet(selected_ws)
                    data = ws.get_all_records()
                    df = pd.DataFrame(data)
        except Exception as e:
            st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã®é–¢æ•°å®šç¾©
def get_embeddings(texts, client):
    """OpenAI APIã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    embeddings = []
    batch_size = 100  # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        try:
            response = client.embeddings.create(
                model="text-embedding-ada-002",
                input=batch
            )
            batch_embeddings = [item.embedding for item in response.data]
            embeddings.extend(batch_embeddings)
        except Exception as e:
            st.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (batch {i//batch_size + 1}): {e}")
            return None
    
    return np.array(embeddings)

def cluster_texts_with_faiss(embeddings, n_clusters=5):
    """FAISSã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
    # æ¬¡å…ƒæ•°
    d = embeddings.shape[1]
    
    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ï¼‰
    # L2æ­£è¦åŒ–ã—ã¦ã‹ã‚‰ã‚¤ãƒ³ãƒŠãƒ¼ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’ä½¿ã†ã“ã¨ã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
    faiss.normalize_L2(embeddings)
    index = faiss.IndexFlatIP(d)
    index.add(embeddings.astype(np.float32))
    
    # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (scikit-learnä½¿ç”¨ã€FAISSã®k-meansã¯è¤‡é›‘ãªãŸã‚)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings)
    
    return cluster_labels, kmeans.cluster_centers_

def create_3d_visualization(embeddings, cluster_labels, texts, sample_size=1000):
    """3æ¬¡å…ƒæ•£å¸ƒå›³ã‚’ä½œæˆ"""
    # ãƒ‡ãƒ¼ã‚¿ãŒå¤šã„å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if len(embeddings) > sample_size:
        indices = np.random.choice(len(embeddings), sample_size, replace=False)
        embeddings_sample = embeddings[indices]
        cluster_labels_sample = cluster_labels[indices]
        texts_sample = [texts[i] for i in indices]
    else:
        embeddings_sample = embeddings
        cluster_labels_sample = cluster_labels
        texts_sample = texts
    
    # PCAã§3æ¬¡å…ƒã«å‰Šæ¸›
    pca = PCA(n_components=3)
    embeddings_3d = pca.fit_transform(embeddings_sample)
    
    # ãƒ—ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    df_plot = pd.DataFrame({
        'x': embeddings_3d[:, 0],
        'y': embeddings_3d[:, 1],
        'z': embeddings_3d[:, 2],
        'cluster': cluster_labels_sample.astype(str),
        'text': [text[:100] + "..." if len(text) > 100 else text for text in texts_sample]
    })
    
    # 3Dæ•£å¸ƒå›³ä½œæˆ
    fig = px.scatter_3d(
        df_plot, 
        x='x', 
        y='y', 
        z='z',
        color='cluster',
        hover_data=['text'],
        title="ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆ3æ¬¡å…ƒå¯è¦–åŒ–ï¼‰",
        labels={'cluster': 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼'}
    )
    
    return fig, pca.explained_variance_ratio_

def summarize_clusters(texts, cluster_labels, client):
    """å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å†…å®¹ã‚’GPTã§è¦ç´„"""
    n_clusters = len(np.unique(cluster_labels))
    summaries = []
    
    for cluster_id in range(n_clusters):
        cluster_texts = [texts[i] for i in range(len(texts)) if cluster_labels[i] == cluster_id]
        cluster_size = len(cluster_texts)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠï¼ˆæœ€å¤§20ä»¶ï¼‰
        sample_texts = cluster_texts[:20] if len(cluster_texts) > 20 else cluster_texts
        
        # GPTã§è¦ç´„
        prompt = f"""
ä»¥ä¸‹ã¯åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åˆ†é¡ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨{cluster_size}ä»¶ä¸­ã®ä»£è¡¨{len(sample_texts)}ä»¶ï¼‰ã§ã™ã€‚
ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å…±é€šã—ãŸç‰¹å¾´ã‚„ä¸»è¦ãªæ„è¦‹ãƒ»ãƒ†ãƒ¼ãƒã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:
{chr(10).join([f"- {text}" for text in sample_texts])}

è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™ï¼š
ä¸»ãªãƒ†ãƒ¼ãƒ: [ãƒ†ãƒ¼ãƒ]
ç‰¹å¾´: [ç‰¹å¾´ã®èª¬æ˜]
ä»£è¡¨çš„ãªæ„è¦‹: [æ„è¦‹ã®è¦ç´„]
"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å…±é€šç‚¹ã‚’è¦‹ã¤ã‘ã¦ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )
            summary = response.choices[0].message.content.strip()
            summaries.append({
                'cluster_id': cluster_id,
                'size': cluster_size,
                'percentage': round(cluster_size / len(texts) * 100, 1),
                'summary': summary
            })
        except Exception as e:
            summaries.append({
                'cluster_id': cluster_id,
                'size': cluster_size,
                'percentage': round(cluster_size / len(texts) * 100, 1),
                'summary': f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            })
    
    return summaries

# å…±é€šå‡¦ç†
if df is not None:
    for col in df.columns:
        if "date" in col.lower() or "time" in col.lower():
            try:
                df[col] = pd.to_datetime(df[col])
            except:
                pass

    st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    st.dataframe(df.head())

    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’æ¤œå‡º
    text_columns = []
    for col in df.columns:
        if df[col].dtype == 'object':  # æ–‡å­—åˆ—å‹ã®åˆ—
            # å¹³å‡æ–‡å­—æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ã‚‹ç¨‹åº¦é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹ï¼‰
            avg_length = df[col].dropna().astype(str).str.len().mean()
            if avg_length > 10:  # å¹³å‡10æ–‡å­—ä»¥ä¸Šã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã¿ãªã™
                text_columns.append(col)

    # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã®è¡¨ç¤º
    if text_columns:
        st.markdown("---")
        st.subheader("ğŸ” ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ")
        
        with st.expander("ğŸ’¡ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ã¯", expanded=False):
            st.markdown("""
**ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½**ã§ã¯ã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®è‡ªç”±è¨˜è¿°ã‚„æ„è¦‹ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§åˆ†æã—ã€ä¼¼ãŸã‚ˆã†ãªå†…å®¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¾ã™ã€‚
- å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸»è¦ãªæ„è¦‹ã®å‚¾å‘ã‚’æŠŠæ¡
- å°‘æ•°æ´¾ã®æ„è¦‹ã‚‚è¦‹é€ƒã•ãšã«å¯è¦–åŒ–
- å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ã‚’è‡ªå‹•ã§è¦ç´„
- 3æ¬¡å…ƒæ•£å¸ƒå›³ã§ç›´æ„Ÿçš„ã«ç†è§£
            """)
        
        selected_text_col = st.selectbox("ğŸ“ åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’é¸æŠ", text_columns)
        n_clusters = st.slider("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°", min_value=2, max_value=10, value=5)
        
        if st.button("ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"):
            openai_api_key = st.secrets.get("OPENAI_API_KEY") or st.text_input("ğŸ”‘ OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›", type="password")
            
            if openai_api_key and selected_text_col:
                client = OpenAI(api_key=openai_api_key)
                
                # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
                text_data = df[selected_text_col].dropna().astype(str).tolist()
                text_data = [text for text in text_data if len(text.strip()) > 5]  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
                
                if len(text_data) < 10:
                    st.warning("âš ï¸ åˆ†æã«ååˆ†ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆæœ€ä½10ä»¶å¿…è¦ï¼‰")
                else:
                    with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­..."):
                        embeddings = get_embeddings(text_data, client)
                    
                    if embeddings is not None:
                        with st.spinner("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­..."):
                            cluster_labels, cluster_centers = cluster_texts_with_faiss(embeddings, n_clusters)
                        
                        # 3æ¬¡å…ƒå¯è¦–åŒ–
                        st.subheader("ğŸ“ˆ 3æ¬¡å…ƒæ•£å¸ƒå›³ã§ã®å¯è¦–åŒ–")
                        fig_3d, explained_variance = create_3d_visualization(embeddings, cluster_labels, text_data)
                        st.plotly_chart(fig_3d, use_container_width=True)
                        
                        st.info(f"ğŸ“Š ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹å¯„ä¸ç‡: {explained_variance[0]:.1%}, {explained_variance[1]:.1%}, {explained_variance[2]:.1%}")
                        
                        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¦ç´„
                        st.subheader("ğŸ“ å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è¦ç´„")
                        with st.spinner("å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åˆ†æä¸­..."):
                            summaries = summarize_clusters(text_data, cluster_labels, client)
                        
                        # çµæœè¡¨ç¤º
                        for summary in sorted(summaries, key=lambda x: x['size'], reverse=True):
                            with st.expander(f"ğŸ·ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {summary['cluster_id'] + 1} ({summary['size']}ä»¶, {summary['percentage']}%)", expanded=True):
                                st.markdown(summary['summary'])
                        
                        # çµ±è¨ˆæƒ…å ±
                        st.subheader("ğŸ“Š åˆ†æçµ±è¨ˆ")
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆæ•°", len(text_data))
                        
                        with col2:
                            st.metric("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°", n_clusters)
                        
                        with col3:
                            largest_cluster_size = max([s['size'] for s in summaries])
                            smallest_cluster_size = min([s['size'] for s in summaries])
                            st.metric("æœ€å¤§/æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼", f"{largest_cluster_size}/{smallest_cluster_size}")

    # æ—¢å­˜ã®æ©Ÿèƒ½ï¼ˆDuckDBå‡¦ç†ï¼‰
    # DuckDBç™»éŒ²
    duck_conn = duckdb.connect()
    duck_conn.register("data", df)

    # ã‚µãƒ³ãƒ—ãƒ«
    with st.expander("ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«è³ªå•ï¼ˆå„ç¨®ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰", expanded=False):
        st.markdown("""
- ã€Œã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å£²ä¸Šã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã¦ã€
- ã€Œæœˆåˆ¥ã®å£²ä¸Šæ¨ç§»ã‚’æ•™ãˆã¦ã€
- ã€Œåœ°åŸŸã”ã¨ã®å£²ä¸Šå‰²åˆã‚’å††ã‚°ãƒ©ãƒ•ã§è¦‹ã›ã¦ã€
- ã€Œæ°—æ¸©ã¨å£²ä¸Šã®é–¢ä¿‚ã‚’æ•£å¸ƒå›³ã§è¦‹ã›ã¦ã€
        """)

    # OpenAI APIã‚­ãƒ¼
    openai_api_key = st.secrets.get("OPENAI_API_KEY") or st.text_input("ğŸ”‘ OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›", type="password")
    user_input = st.chat_input("è‡ªç„¶è¨€èªã§è³ªå•ã—ã¦ãã ã•ã„")

    if user_input and openai_api_key:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            with st.spinner("åˆ†æä¸­..."):
                client = OpenAI(api_key=openai_api_key)

                schema_desc = "\n".join([f"{col} ({dtype})" for col, dtype in zip(df.columns, df.dtypes)])
                sample_data = df.head(3).to_string() 
                prompt = f"""
ã‚ãªãŸã¯DuckDBã«å¯¾ã—ã¦SQLã‚’ç”Ÿæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

åŸºæœ¬ãƒ«ãƒ¼ãƒ«:
- ãƒ†ãƒ¼ãƒ–ãƒ«åã¯ `data` ã§ã™
- ã‚ã‚‹ç¨‹åº¦æ›–æ˜§ãªè³ªå•ã«å¯¾ã—ã¦ã‚‚ã‚«ãƒ©ãƒ ã‚’äºˆæ¸¬ã—ã¦SQLã‚’ç™ºè¡Œã—ã¦ãã ã•ã„
- å‡ºåŠ›ã¯SQLæ–‡ã®ã¿ã€‚ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„è£…é£¾ã¯ä¸è¦ã§ã™

é‡è¦ãªæ³¨æ„äº‹é …:
1. **åˆ—åã®æ­£ç¢ºæ€§**: åˆ—åã¯ä¸‹è¨˜ã‚¹ã‚­ãƒ¼ãƒã¨å®Œå…¨ã«ä¸€è‡´ã•ã›ã¦ãã ã•ã„ï¼ˆæ‹¬å¼§ã‚„ç‰¹æ®Šæ–‡å­—ã‚‚å«ã‚ã¦æ­£ç¢ºã«ï¼‰
2. **æ—¥ä»˜å‡¦ç†**: DuckDBã§ã¯æ–‡å­—åˆ—ã‚’æ—¥ä»˜é–¢æ•°ã«ä½¿ã†å ´åˆã€å¿…ãš `CAST(åˆ— AS DATE)` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
   ãŸã ã—ã€æ—¥ä»˜å‹ã§ãªã„åˆ—ï¼ˆæ–‡å­—åˆ—ã€æ•°å€¤ãªã©ï¼‰ã«å¯¾ã—ã¦ã¯CASTã—ãªã„ã§ãã ã•ã„
3. **ç›¸é–¢ãƒ»é–¢ä¿‚æ€§åˆ†æ**: ã€Œé–¢ä¿‚ã€ã€Œç›¸é–¢ã€ã€Œé–¢é€£ã€ãªã©ã®è³ªå•ã§ã¯ã€`SELECT col1, col2 FROM data` ã®ã‚ˆã†ã«2åˆ—ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€çµæœã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæ•£å¸ƒå›³æç”»ã®ãŸã‚ï¼‰
4. **ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª**: ä¸‹è¨˜ã®ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‚è€ƒã«ã€é©åˆ‡ãªåˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„
5. **æ–‡å­—åˆ—æ¯”è¼ƒ**: æ–‡å­—åˆ—ã®æ¯”è¼ƒã§ã¯ç­‰å·ï¼ˆ=ï¼‰ã‚„LIKEã‚’ä½¿ã„ã€ä¸é©åˆ‡ãªCASTã¯é¿ã‘ã¦ãã ã•ã„

ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ:
{schema_desc}

ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå‚è€ƒï¼‰:
{sample_data}

è³ªå•: {user_input}
"""
                try:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "ã‚ãªãŸã¯SQLã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"},
                            {"role": "user", "content": prompt}
                        ]
                    )

                    raw_sql = response.choices[0].message.content.strip()
                    sql = re.sub(r"```sql|```", "", raw_sql).strip()
                    st.markdown(f"ğŸ§  **ç”Ÿæˆã•ã‚ŒãŸSQL:**\n```sql\n{sql}\n```")

                    result_df = duck_conn.execute(sql).fetchdf()
                    st.dataframe(result_df)

                    if result_df.shape[1] >= 2:
                        x, y = result_df.columns[0], result_df.columns[1]

                        q = user_input.lower()
                        if any(w in q for w in ["å‰²åˆ", "æ¯”ç‡", "ã‚·ã‚§ã‚¢", "å††"]):
                            chart_type = "pie"
                        elif any(w in q for w in ["ç›¸é–¢", "é–¢ä¿‚", "é–¢é€£"]):
                            chart_type = "scatter"
                        elif any(w in q for w in ["æ™‚é–“", "æ—¥æ™‚", "æ¨ç§»", "å‚¾å‘", "æœˆ", "æ—¥", "æ™‚ç³»åˆ—"]):
                            chart_type = "line"
                        else:
                            chart_type = "bar"

                        try:
                            result_df[x] = pd.to_datetime(result_df[x])
                        except:
                            pass

                        if chart_type == "pie":
                            st.info("ğŸ“Š å††ã‚°ãƒ©ãƒ•ã§ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å‰²åˆã‚’å¯è¦–åŒ–ã—ã¦ã„ã¾ã™")
                            fig = px.pie(result_df, names=x, values=y)
                        elif chart_type == "scatter":
                            st.info("ğŸ“ˆ æ•£å¸ƒå›³ã§2ã¤ã®æ•°å€¤ã®é–¢ä¿‚æ€§ã‚’è¦–è¦šåŒ–ã—ã¦ã„ã¾ã™ã€‚")
                            result_df[x] = pd.to_numeric(result_df[x], errors='coerce')
                            result_df[y] = pd.to_numeric(result_df[y], errors='coerce')
                            fig = px.scatter(result_df, x=x, y=y)
                            if pd.api.types.is_numeric_dtype(result_df[x]) and pd.api.types.is_numeric_dtype(result_df[y]):
                                try:
                                    corr = np.corrcoef(result_df[x], result_df[y])[0, 1]
                                    st.success(f"ğŸ“Š **ç›¸é–¢ä¿‚æ•°**: `{corr:.3f}`")
                                except:
                                    st.warning("âš ï¸ ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        elif chart_type == "line":
                            st.info("ğŸ“ˆ æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§æ™‚ç³»åˆ—ã®æ¨ç§»ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                            fig = px.line(result_df, x=x, y=y)
                        else:
                            st.info("ğŸ“Š æ£’ã‚°ãƒ©ãƒ•ã§ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ¯”è¼ƒã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                            fig = px.bar(result_df, x=x, y=y)

                        st.plotly_chart(fig, use_container_width=True)

                        # ğŸ” AIã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•è¦ç´„
                        summary_prompt = f"""
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€Œ{chart_type}ã€ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€Œ{user_input}ã€ã«å¯¾ã™ã‚‹çµæœã§ã™ã€‚
ã“ã®çµæœã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«çµè«–ã‹ã‚‰ç­”ãˆã‚‹ã‚ˆã†ã«ã€æ—¥æœ¬èªã§ç°¡æ½”ã«æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«5è¡Œãã‚‰ã„ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{result_df.head(20).to_csv(index=False)}
"""
                        try:
                            summary_response = client.chat.completions.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®å°‚é–€å®¶ã§ã€ã‚°ãƒ©ãƒ•ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å†…å®¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¾ã™ã€‚"},
                                    {"role": "user", "content": summary_prompt}
                                ]
                            )
                            summary_text = summary_response.choices[0].message.content.strip()
                            st.markdown("ğŸ“ **ã‚°ãƒ©ãƒ•ã®è¦ç´„:**")
                            st.success(summary_text)

                        except Exception as e:
                            st.warning(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

                    else:
                        st.info("ğŸ“‰ ã‚°ãƒ©ãƒ•æç”»ã«ã¯2åˆ—ä»¥ä¸Šã®çµæœãŒå¿…è¦ã§ã™ã€‚")

                except Exception as e:
                    st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ãŸãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
                    error_str = str(e)
                    if "not found in FROM clause" in error_str:
                        st.info("ğŸ’¡ **åˆ—åã‚¨ãƒ©ãƒ¼**: ä»¥ä¸‹ã®åˆ—åã‚’å‚è€ƒã«ã€æ­£ç¢ºãªåˆ—åã§è³ªå•ã—ã¦ãã ã•ã„")
                        col_list = "ã€".join([f"`{col}`" for col in df.columns])
                        st.markdown(f"**åˆ©ç”¨å¯èƒ½ãªåˆ—**: {col_list}")
                    elif "CAST" in error_str:
                        st.info("ğŸ’¡ **ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼**: æ—¥ä»˜ã§ãªã„åˆ—ã‚’æ—¥ä»˜ã¨ã—ã¦å‡¦ç†ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™")
                    else:
                        st.info("ğŸ’¡ è³ªå•ã‚’å°‘ã—å¤‰ãˆã¦ã€ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„")
                    
                    # å‚è€ƒç”¨ã«ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
                    st.markdown("**ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå‚è€ƒï¼‰:**")
                    st.dataframe(df.head())
